<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project
  | ECE, Virginia Tech | Fall 2015: ECE 5554/4984</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Hand Pose Estimation by Fusion of Multi-View Images</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Avinash Vemuri, David Wen, Tan Gemicioglu</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2020 CS4476 Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<figure style="text-align: center;">
  <img style="height: 300px; text-align: center;" alt="" src="./images/stereo_fusion2.png">
  <figcaption style="text-align: center;">Fig 1. Architecture for proposed stereo fusion based hand pose estimation model.</figcaption>
</figure>
<br>
<!-- Statement -->
<h3>Abstract</h3>
Hand pose estimation is an important application of computer vision used for gesture recognition and hand tracking in videos, user input systems and virtual reality. The primary approaches for hand pose estimation rely on single RGB images, which often give inaccurate results or RGBD (RGB + depth) images which require the user to have expensive, specialized cameras. Recently, there have been advances in using two RGB images taken at the same time for stereoscopic vision as an alternative to perceiving depth with RGBD. We build on past attempts at using stereoscopic vision for hand pose estimation by utilizing a type of CNN that creates an early fusion of the two RGB images. We aim to use this improvement to obtain a better generalization of the stereoscopy and higher accuracies in the pose estimates.
<br><br>

<!-- Introduction -->
<h3>Introduction</h3>
<figure style="text-align: center;">
  <img style="height: 300px; " alt="" src="./images/joints.png">
  <figcaption style="text-align: center;">Fig 2. Visual of keypoint hand model with 14, 16, and 21 joints [1].</figcaption>
</figure>
<br>
Estimating and tracking hand poses has been one of the most critical tools for following human activity in human computer interaction since it became viable. It’s been used for tracking motion in videos, sign language recognition and as input systems for virtual and augmented reality [1]. Especially with developments in virtual reality, it has become more and more important to be able to recognize hands and estimate their poses with consumer-level hardware.
<br><br>
The goal of hand pose estimation is to find a number of keypoints marking important spots on the hand. There are different models for the joints of the hand with varying number of keypoints but the 21-joint model is currently the most commonly used approach. A good hand pose estimation allows for stable, accurate and consistent models for gesture recognition and hand tracking.
<br><br>
After starting with RGB images, substantial improvements have been made by using RGBD cameras which include depth maps in addition to the colors. However, these instruments are more expensive than traditional RGB cameras and can be difficult to set up. As an alternative, improvements in the field of object recognition have shown that it is possible to estimate depth in images using stereoscopic 3D effects [3]. Some recent attempts have shown that this usage of stereoscopic vision performs successfully when it comes to hand pose estimation as well [2].
<br><br>
We build on previous work in estimating hand poses using stereoscopic vision by utilizing a new kind of image fusing technique that happens at an early stage in the neural network as an alternative to traditional late geometric fusion. This new fusion technique allows more information to be used from both images used as an input by leaving the judgement of how it should combine the two to the neural network.


<figure style="text-align: center;">
  <img style="height: 300px; " alt="" src="./images/hands.png">
  <figcaption style="text-align: center;">Fig 3. Hands with keypoints and visualized hand model [4].</figcaption>
</figure>

<br><br>
<!-- Approach -->
<h3>Approach</h3>

For our approach, the dataset from J. Zhang et al. in [2] is formatted to stereo image pairs with 21 UVD keypoints as labels.  Using the given camera calibration parameters and XYZ coordinate labels, the new UVD keypoints are created by projecting the XYZ keypoints onto the left image.  The existing approach by J. Zhang et al. uses an adaptive GNN trained on Google images to create a probability map from the left image, which they then use with the explicit disparity map to create a final hand segmentation map to estimate the hand pose keypoints.  
<br><br>
For our implementation, we preprocess the stereo images using the skin color segmentation method outlined by R. F. Rahmat et al. in [5], and instead of using an explicit disparity map, we perform early fusion of the stereo images to let the deep CNN implicitly determine disparity.  The deep CNN generates 21 probability maps for each keypoint, from which an integral image is computed to determine optimal UVD keypoints.  
<br><br>
For the training of the model, we use the three loss functions as shown in the figure below.
<br>
<figure>
  <img style="height: 200px;" alt="" src="./images/loss.png">
  <figcaption style="text-align: center;">Fig 4. Loss function proposed by X. Liu et al. in [3]</figcaption>
</figure>
<br>
The first term is the traditional loss function, where the error is the squared error between the computed UVD and the labeled UVD.  The second term is the squared error between the reprojection of the predicted UVD keypoints and the labeled UVD keypoints.  The third term is the locality loss of the predicted UVD keypoints and the labeled UVD keypoints, using the inverse normal distribution centered at the labeled UVD and the probability of UVD in the generated probability map.  Optimal weights for the last two terms will be determined by future experimentation.

<br><br>
<!-- Experiment and Results -->
<h3>Experiment and Results</h3>
We used the dataset provided in source [2]. The data set is made up of 18,000 stereo image pairs and 18,000 depth images taken in 6 different scenarios and the ground-truth 3D positions of the palm and finger joints.  Currently, the deep CNN model is a work in progress, so we do not have any numerical results or metrics.  In the future, we plan on testing our models reliability in placing keypoints on hands in a variety of backgrounds and lighting. 
<br><br>
For our future experimentation, we are planning on doing a traditional 60/20/20 split for the dataset with 60% in our training set, 20% in validation, and 20% in testing. Additionally, we believe it will be a useful experiment for generalization to use the 6 different background scenes in our dataset by doing 6-fold cross validation with training on 5 scenes and testing on 1 scene.


<br><br>
<!-- Qualitative Results -->
<h3>Qualitative Results</h3>
For the preprocessing step, the results for the skin segmentation for select images in different environments are shown below:
<br>
<figure style="text-align: center;">
  <div class="row">
    <div class="column">
      <img src="./images/mask_applied_img1.png" alt="" style="width:100%">
    </div>
    <div class="column">
      <img src="./images/mask_applied_img2.png" alt="" style="width:100%">
    </div>
    <div class="column">
      <img src="./images/mask_applied_img3.png" alt="" style="width:100%">
    </div>
    <div class="column">
      <img src="./images/mask_applied_img4.png" alt="" style="width:100%">
    </div>
    <div class="column">
      <img src="./images/mask_applied_img5.png" alt="" style="width:100%">
    </div>
    <div class="column">
      <img src="./images/mask_applied_img6.png" alt="" style="width:100%">
    </div>
  </div>
  <figcaption style="text-align: center;">Fig 5. Qualitative results from skin segmentation.</figcaption>
</figure>

<br>
Currently, the skin segmentation method does not handle passive lighting very well (as shown in the third image in Fig 5.).  For the other environments, the hand is preserved fairly well.  Other skin features, such as the face of the researcher (in images 1-3 of Fig 5.) are moderately preserved and may affect the model, and additional preprocessing steps to remove the face may be beneficial.  Common to all the skin segmentations is the abundance of holes and noise in the resultant images.  Further preprocessing steps to eliminate holes and noise from the binary image mask either through some combination of dilation and erosion or contour filling will be added to further improve the robustness of the segmentation in different backgrounds and lighting conditions.

<br><br>
<!-- Conclusion and Future Work  -->
<h3>Conclusion and Future Work</h3>
With this project, we investigated a stereoscopic vision alternative to the current dominant paradigms of single RGB or RGBD cameras in hand pose estimation. We made use of new ideas in object recognition that use neural networks instead of geometric calculation to gain more information from the stereoscopic fusion of two images. We hope that this approach will make techniques that utilize hand pose estimation more accessible by reducing the need for costly equipment and more accurate by utilizing stereoscopic information. 
<br><br>
At the moment, datasets for stereoscopic vision in hand pose estimation are very limited. For future work, larger datasets with more variation in scene as well as artificially generated images could help make better models. The dataset we used made use of a stereoscopic camera, which is a far more controlled condition than doing stereoscopic fusion for two different cameras. Cameras with varying degrees of overlap and different configurations would be useful in making it easier to deploy stereoscopic vision at the consumer level.
<br><br>
We also investigated different preprocessing techniques, and further work on different preprocessing approaches as well as neural network approaches could lead to further improvements in this area. Finally, we believe that the most critical improvement for hand pose estimation with stereoscopic vision would be in making models as light and fast as possible for them to work with real time videos across several cameras.

<br><br>
<!-- References -->
<h3>References</h3>
<ol>
  <li>
    <p>B. Doosti, “Hand Pose Estimation: A Survey,” <cite><a href="http://arxiv.org/abs/1903.01013">arXiv:1903.01013</a></cite> [cs], Jun. 2019.</p>
  </li>
  <li>
    <p>J. Zhang, J. Jiao, M. Chen, L. Qu, X. Xu, and Q. Yang, “3D Hand Pose Tracking and Estimation Using Stereo Matching,” <cite><a href="http://arxiv.org/abs/1610.07214">arXiv:1610.07214</a></cite> [cs], Oct. 2016.</p>
  </li>
  <li>
    <p>X. Liu, R. Jonschkowski, A. Angelova, and K. Konolige, “KeyPose: Multi-View 3D Labeling and Keypoint Estimation for Transparent Objects,” <cite><a href="http://arxiv.org/abs/1912.02805">arXiv:1912.02805</a></cite> [cs], May 2020.</p>
  </li>
  <li>
    <p>F. Zhang, V. Bazarevsky, A. Vakunov, A. Tkachenka, G. Sung, C.-L. Chang, and M. Grundmann, “MediaPipe Hands: On-device Real-time Hand Tracking,” <cite><a href="https://arxiv.org/abs/2006.10214">arXiv:2006.10214</a></cite> [cs], Jun. 2020.</p>
  </li>
  <li>
      <p>R. F. Rahmat, T. Chairunnisa, D. Gunawan and O. S. Sitompul, "Skin color segmentation using multi-color space threshold," 2016 3rd International Conference on Computer and Information Sciences (ICCOINS), Kuala Lumpur, 2016, pp. 391-396, doi: 10.1109/ICCOINS.2016.7783247.</p>
  </li>
</ol>

</body></html>